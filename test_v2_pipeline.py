"""
Proof-of-Concept: Complete V2 Pipeline Test

Demonstrates the correct flow:
1. Load audio analysis (real transcript with timestamps)
2. Detect opportunities (extract REAL quotes)
3. Load frame evidence (visual descriptions)
4. Generate questions (integrate audio + visual)
5. Validate questions (check all guidelines)
6. Compare with old output

Usage:
    python test_v2_pipeline.py <video_id>

Example:
    python test_v2_pipeline.py "video_20251118_133434_Copy of VOmj6qaznos"
"""

import sys
import json
from pathlib import Path
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import our V2 modules
from processing.opportunity_detector_v2 import OpportunityDetectorV2
from processing.multimodal_question_generator_v2 import MultimodalQuestionGeneratorV2


def load_json(filepath: Path) -> dict:
    """Load JSON file"""
    logger.info(f"Loading: {filepath}")
    with open(filepath, 'r') as f:
        return json.load(f)


def save_json(data: dict, filepath: Path):
    """Save JSON file"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    logger.info(f"Saved: {filepath}")


def compare_old_vs_new(old_questions: dict, new_questions: dict):
    """
    Compare old (broken) questions with new (fixed) questions.

    Shows key differences:
    - Old: synthetic quotes, timestamp 0, missing integration
    - New: real quotes, actual timestamps, validated integration
    """
    print("\n" + "=" * 80)
    print("COMPARISON: OLD vs NEW")
    print("=" * 80)

    old_q_list = old_questions.get("questions", [])
    new_q_list = new_questions.get("questions", [])

    print(f"\nTotal Questions:")
    print(f"  Old: {len(old_q_list)}")
    print(f"  New: {len(new_q_list)}")

    print(f"\nValidated Questions:")
    print(f"  Old: N/A (no validation)")
    print(f"  New: {new_questions.get('validated_questions', 0)}")

    # Compare first question as example
    if old_q_list and new_q_list:
        print("\n" + "-" * 80)
        print("EXAMPLE COMPARISON - First Question:")
        print("-" * 80)

        old_q1 = old_q_list[0]
        new_q1 = new_q_list[0]

        print("\nüìã OLD QUESTION (BROKEN):")
        print(f"  Question: {old_q1.get('question', '')[:100]}...")
        print(f"  Audio Cue: {old_q1.get('audio_cue', '')[:60]}...")
        print(f"  Timestamps: {old_q1.get('start_timestamp')} - {old_q1.get('end_timestamp')}")
        print(f"  ‚ùå Issues:")
        print(f"     - Audio cue likely synthetic (not in transcript)")
        print(f"     - Timestamps likely wrong (all 00:00:00 or 19:01)")
        print(f"     - Visual description copy-pasted")

        print("\n‚úÖ NEW QUESTION (FIXED):")
        print(f"  Question: {new_q1.get('question', '')[:100]}...")
        print(f"  Audio Cue: {new_q1.get('audio_cue', '')[:60]}...")
        print(f"  Timestamps: {new_q1.get('start_timestamp')} - {new_q1.get('end_timestamp')}")
        print(f"  Validated: {new_q1.get('validated', False)}")
        print(f"  ‚úÖ Improvements:")
        print(f"     - Audio cue from REAL transcript")
        print(f"     - Timestamps match audio position")
        print(f"     - Visual description from actual frame")
        print(f"     - Both modalities required")
        print(f"     - No names used")


def test_v2_pipeline(video_id: str, output_base: Path):
    """
    Test V2 pipeline end-to-end.

    Args:
        video_id: Video ID to process
        output_base: Base outputs directory
    """
    print("\n" + "=" * 80)
    print("V2 PIPELINE TEST - PROOF OF CONCEPT")
    print("=" * 80)
    print(f"Video ID: {video_id}")

    # Determine output directory
    output_dir = output_base / video_id

    # Extract base name from video_id
    # Format: video_YYYYMMDD_HHMMSS_OriginalName
    if video_id.startswith("video_"):
        parts = video_id.replace("video_", "", 1).split("_", 2)
        if len(parts) >= 3:
            date_part = parts[0]
            time_part = parts[1]
            original_name = parts[2]
            base_name = f"{original_name}_{date_part}_{time_part}"
        else:
            base_name = video_id
    else:
        base_name = video_id

    # Define file paths
    audio_file = output_dir / f"{base_name}_audio_analysis.json"
    evidence_file = output_dir / f"{base_name}_evidence.json"
    old_questions_file = output_dir / f"{base_name}_questions.json"

    # New output files
    opportunities_v2_file = output_dir / f"{base_name}_opportunities_v2.json"
    questions_v2_file = output_dir / f"{base_name}_questions_v2.json"

    # Check files exist
    print("\nüìÅ Checking input files...")
    for filepath in [audio_file, evidence_file]:
        if not filepath.exists():
            print(f"‚ùå Missing: {filepath}")
            return False
        print(f"‚úÖ Found: {filepath.name}")

    if old_questions_file.exists():
        print(f"‚úÖ Found old questions: {old_questions_file.name}")
    else:
        print(f"‚ö†Ô∏è  No old questions file (will skip comparison)")

    # STEP 1: Load audio analysis
    print("\n" + "=" * 80)
    print("STEP 1: Load Audio Analysis")
    print("=" * 80)
    audio_analysis = load_json(audio_file)
    segments = audio_analysis.get("segments", [])
    duration = audio_analysis.get("duration", 0.0)
    print(f"Segments: {len(segments)}")
    print(f"Duration: {duration:.1f}s")

    # Show sample segments
    print("\nSample segments (first 3):")
    for i, seg in enumerate(segments[:3]):
        print(f"  [{seg['start']:.2f}s] {seg['text']}")

    # STEP 2: Detect opportunities (V2)
    print("\n" + "=" * 80)
    print("STEP 2: Detect Real Opportunities")
    print("=" * 80)
    detector = OpportunityDetectorV2()
    opportunities_result = detector.detect_opportunities(audio_analysis, video_id)
    detector.save_opportunities(opportunities_result, opportunities_v2_file)

    print(f"\nOpportunities detected: {opportunities_result.validated_opportunities}")
    if opportunities_result.opportunities:
        print("\nSample opportunity:")
        opp = opportunities_result.opportunities[0]
        print(f"  Type: {opp.opportunity_type}")
        print(f"  Audio: \"{opp.audio_quote[:60]}...\"")
        print(f"  Timestamp: {opp.audio_start:.2f}s - {opp.audio_end:.2f}s")
        print(f"  Validated: {opp.validated_audio}")

    # STEP 3: Load evidence
    print("\n" + "=" * 80)
    print("STEP 3: Load Frame Evidence")
    print("=" * 80)
    evidence = load_json(evidence_file)
    frames = evidence.get("frames", {})
    print(f"Frames: {len(frames)}")

    # STEP 4: Generate questions (V2)
    print("\n" + "=" * 80)
    print("STEP 4: Generate Multimodal Questions")
    print("=" * 80)
    generator = MultimodalQuestionGeneratorV2()
    questions_result = generator.generate_questions(
        opportunities=opportunities_result.to_dict(),
        evidence=evidence,
        audio_analysis=audio_analysis,
        video_id=video_id,
        target_count=30
    )
    generator.save_questions(questions_result, questions_v2_file)

    print(f"\nQuestions generated: {questions_result.total_questions}")
    print(f"Validated questions: {questions_result.validated_questions}")

    if questions_result.questions:
        print("\nSample question:")
        q = questions_result.questions[0]
        print(f"  Q: {q.question[:80]}...")
        print(f"  Audio cue: \"{q.audio_cue[:50]}...\"")
        print(f"  Timestamps: {q.start_timestamp} - {q.end_timestamp}")
        print(f"  Validated: {q.validated}")

    # STEP 5: Compare with old
    if old_questions_file.exists():
        print("\n" + "=" * 80)
        print("STEP 5: Compare with Old Questions")
        print("=" * 80)
        old_questions = load_json(old_questions_file)
        compare_old_vs_new(old_questions, questions_result.to_dict())

    # Summary
    print("\n" + "=" * 80)
    print("‚úÖ V2 PIPELINE TEST COMPLETE")
    print("=" * 80)
    print(f"Output files:")
    print(f"  - {opportunities_v2_file}")
    print(f"  - {questions_v2_file}")
    print("\nKey improvements:")
    print("  ‚úÖ Real quotes from transcript (not synthetic)")
    print("  ‚úÖ Actual timestamps from segments")
    print("  ‚úÖ Visual descriptions from correct frames")
    print("  ‚úÖ Both audio + visual cues")
    print("  ‚úÖ Validation checks passed")
    print("  ‚úÖ No names used")
    print("=" * 80)

    return True


if __name__ == "__main__":
    if len(sys.argv) > 1:
        video_id = sys.argv[1]
        output_base = Path("/Users/aranja14/Desktop/Gemini_QA/outputs")

        success = test_v2_pipeline(video_id, output_base)

        if success:
            print("\nüéâ Test completed successfully!")
            sys.exit(0)
        else:
            print("\n‚ùå Test failed!")
            sys.exit(1)
    else:
        print("Usage: python test_v2_pipeline.py <video_id>")
        print("\nExample:")
        print('  python test_v2_pipeline.py "video_20251118_133434_Copy of VOmj6qaznos"')
        sys.exit(1)
