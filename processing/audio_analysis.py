"""
Audio Analysis Module - Whisper + pyannote Integration (OPTIMIZED)

Extracts audio transcript with speaker diarization for intelligent
frame selection and genre detection.

Phase 1 of the new evidence-first architecture.

ENHANCEMENTS:
- Word-level timestamps enabled for precise temporal alignment
- Audio event detection (music, sound effects, crowd sounds)
- Music tempo/tone change detection
- Silence gap detection for scene changes
- HH:MM:SS timestamp formatting for validation

OPTIMIZATIONS:
- Selective audio event detection (higher quality thresholds)
- Skip low-intensity/short-duration events
- Focus on meaningful audio transitions
- Reduce noise from trivial sound effects
"""

import os
# Fix for OMP threading issues when running in background tasks
os.environ["OMP_NUM_THREADS"] = "1"

import logging
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import subprocess
import tempfile
from datetime import timedelta

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenv not installed, skip

logger = logging.getLogger(__name__)


class AudioAnalyzer:
    """
    Audio analysis using Whisper (transcription) + pyannote (speaker diarization)
    
    OPTIMIZED for quality over quantity in audio event detection.
    """
    
    # Audio Event Detection Thresholds (optimized for quality)
    SOUND_EFFECT_MIN_RMS = 0.20  # Increased from 0.1 (skip low-intensity sounds)
    SOUND_EFFECT_MIN_DURATION = 0.5  # Minimum duration in seconds
    CROWD_SOUND_MIN_RMS = 0.15  # Increased from 0.05 (only significant crowd sounds)
    MUSIC_DETECTION_WINDOW = 5.0  # Seconds for intro/outro detection
    MUSIC_MIN_TEMPO_CHANGE = 20  # BPM change threshold

    def __init__(self, video_path: str):
        """
        Initialize audio analyzer

        Args:
            video_path: Path to video file
        """
        self.video_path = Path(video_path)
        self.audio_path: Optional[Path] = None
        self.transcript: Optional[Dict] = None
        self.diarization: Optional[Dict] = None

        if not self.video_path.exists():
            raise FileNotFoundError(f"Video not found: {self.video_path}")

        logger.info(f"AudioAnalyzer initialized for: {self.video_path.name}")
        logger.info(f"  OPTIMIZED audio event detection (quality over quantity)")
        logger.info(f"  Sound effect min RMS: {self.SOUND_EFFECT_MIN_RMS}")
        logger.info(f"  Sound effect min duration: {self.SOUND_EFFECT_MIN_DURATION}s")
        logger.info(f"  Crowd sound min RMS: {self.CROWD_SOUND_MIN_RMS}")

    def extract_audio(self, output_dir: Optional[Path] = None) -> Path:
        """
        Extract audio track from video file using ffmpeg

        Args:
            output_dir: Directory to save audio (default: temp dir)

        Returns:
            Path to extracted audio file (WAV format)
        """
        logger.info("=" * 80)
        logger.info("EXTRACTING AUDIO FROM VIDEO")
        logger.info("=" * 80)

        if output_dir is None:
            output_dir = Path(tempfile.gettempdir())

        # Create output filename
        audio_filename = f"{self.video_path.stem}_audio.wav"
        self.audio_path = output_dir / audio_filename

        logger.info(f"Input video: {self.video_path}")
        logger.info(f"Output audio: {self.audio_path}")

        # Use ffmpeg to extract audio with enhancement filters
        # -vn: no video
        # -acodec pcm_s16le: 16-bit PCM
        # -ar 16000: 16kHz sample rate (optimal for Whisper)
        # -ac 1: mono channel
        # -af: audio filters for better transcription:
        #   - highpass=f=200: Remove low-frequency rumble
        #   - lowpass=f=3000: Remove high-frequency noise
        #   - volume=2: Boost volume for better recognition
        #   - dynaudnorm: Dynamic audio normalization
        cmd = [
            "ffmpeg",
            "-i", str(self.video_path),
            "-vn",  # No video
            "-acodec", "pcm_s16le",  # 16-bit PCM
            "-ar", "16000",  # 16kHz
            "-ac", "1",  # Mono
            "-af", "highpass=f=200,lowpass=f=3000,volume=2,dynaudnorm",  # Audio enhancement
            "-y",  # Overwrite
            str(self.audio_path)
        ]

        try:
            logger.info("Running ffmpeg with audio enhancement filters...")
            logger.info("  - Removing low-frequency rumble (highpass)")
            logger.info("  - Removing high-frequency noise (lowpass)")
            logger.info("  - Boosting volume for better recognition")
            logger.info("  - Normalizing audio dynamics")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5 min timeout
            )

            if result.returncode != 0:
                logger.error(f"ffmpeg failed: {result.stderr}")
                raise RuntimeError(f"Audio extraction failed: {result.stderr}")

            file_size = self.audio_path.stat().st_size / 1024 / 1024  # MB
            logger.info(f"âœ“ Audio extracted: {file_size:.2f} MB")
            return self.audio_path

        except subprocess.TimeoutExpired:
            logger.error("ffmpeg timeout after 5 minutes")
            raise RuntimeError("Audio extraction timeout")
        except Exception as e:
            logger.error(f"Audio extraction error: {e}")
            raise

    # Known Whisper hallucinations (common phrases when no speech present)
    HALLUCINATIONS = [
        "i'm sorry",
        "i don't know",
        "thank you",
        "thank you for watching",
        "please subscribe",
        "thanks for watching",
        "bye",
        "goodbye",
        "see you next time",
        "Â¶Â¶",  # Musical note symbols
        "â™ªâ™ª",  # Alternative musical notes
        "...",  # Ellipsis (silence)
        "[music]",
        "[silence]"
    ]

    def _is_copyright_watermark(self, text: str) -> bool:
        """Check if text looks like a copyright/watermark hallucination"""
        import re
        text_lower = text.lower()

        # Check for copyright symbols
        if 'Â©' in text or 'Â®' in text or 'â„¢' in text:
            return True

        # Check for year patterns (2000-2099) with copyright-like context
        if re.search(r'\b(20\d{2})\b', text):
            # Common copyright/watermark keywords
            watermark_keywords = ['copyright', 'rights reserved', 'all rights',
                                   'tv', 'production', 'media', 'broadcast',
                                   'inc', 'llc', 'ltd', 'corp']
            if any(keyword in text_lower for keyword in watermark_keywords):
                return True

        # Check for "Subtitle by" or similar
        if any(phrase in text_lower for phrase in ['subtitle by', 'subtitles by',
                                                     'translated by', 'captioned by']):
            return True

        return False

    def transcribe_with_whisper(self, audio_path: Optional[Path] = None) -> Dict:
        """
        Transcribe audio using Whisper with hallucination filtering

        Args:
            audio_path: Path to audio file (default: use extracted audio)

        Returns:
            Transcript dict with segments and timestamps (filtered for hallucinations)
        """
        logger.info("=" * 80)
        logger.info("TRANSCRIBING WITH WHISPER (WITH HALLUCINATION FILTERING)")
        logger.info("=" * 80)

        if audio_path is None:
            if self.audio_path is None:
                raise ValueError("No audio file available. Run extract_audio() first.")
            audio_path = self.audio_path

        try:
            import whisper

            # Auto-detect best device for Whisper
            import torch
            if torch.cuda.is_available():
                device = "cuda"
                logger.info(f"Using CUDA GPU: {torch.cuda.get_device_name(0)} for Whisper")
            else:
                device = "cpu"
                logger.info("Using CPU for Whisper (no GPU available)")

            # Load Whisper model (large-v3 for best accuracy, especially for noisy audio)
            logger.info("Loading Whisper large-v3 model (this may take a moment)...")
            model = whisper.load_model("large-v3", device=device)

            # Transcribe with word-level timestamps (required for adversarial pipeline)
            logger.info(f"Transcribing: {audio_path.name}")
            logger.info("â³ Transcription in progress... (this may take several minutes)")
            logger.info("ðŸ’¡ Tip: Check console output for real-time progress bar")

            result = model.transcribe(
                str(audio_path),
                language="en",  # Can be auto-detected if needed
                task="transcribe",
                verbose=True,  # Enable to see transcription progress
                word_timestamps=True,  # ENABLED for precise temporal alignment (required for adversarial questions)
                condition_on_previous_text=False  # Disable to reduce hallucinations
            )

            # Extract full transcript text
            full_text = result["text"].strip()

            # Extract segments with word-level timestamps and hallucination filtering
            segments = []
            filtered_count = 0

            for segment in result["segments"]:
                segment_text = segment["text"].strip()
                no_speech_prob = segment.get("no_speech_prob", 0.0)

                # Check for hallucinations
                is_hallucination = segment_text.lower() in self.HALLUCINATIONS
                is_likely_no_speech = no_speech_prob > 0.6
                is_too_short = len(segment_text) < 3
                is_only_symbols = bool(segment_text) and all(c in 'Â¶â™ª.,!? \n\t-_' for c in segment_text)
                is_watermark = self._is_copyright_watermark(segment_text)

                # Filter and replace with descriptive placeholder
                if is_hallucination or is_likely_no_speech or is_too_short or is_only_symbols or is_watermark:
                    if is_hallucination or is_only_symbols or is_watermark:
                        segment_text = "[Music or background audio]"
                        filtered_count += 1
                    elif is_likely_no_speech:
                        segment_text = "[No speech detected]"
                        filtered_count += 1
                    elif is_too_short:
                        segment_text = "[Brief audio]"
                        filtered_count += 1

                # Extract word timestamps if available
                word_timestamps = []
                if "words" in segment and segment["words"]:
                    for word in segment["words"]:
                        word_timestamps.append({
                            "word": word.get("word", ""),
                            "start": word.get("start", 0.0),
                            "end": word.get("end", 0.0),
                            "probability": word.get("probability", 1.0),
                            # Add HH:MM:SS formatted timestamps for easy validation
                            "start_time": self._format_timestamp(word.get("start", 0.0)),
                            "end_time": self._format_timestamp(word.get("end", 0.0))
                        })

                segments.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment_text,
                    "speaker": None,  # Will be filled by diarization
                    "words": word_timestamps,  # Word-level timestamps for opportunity mining
                    "no_speech_prob": no_speech_prob,  # Store for downstream filtering
                    "is_filtered": is_hallucination or is_likely_no_speech or is_too_short or is_only_symbols or is_watermark,
                    # Add HH:MM:SS formatted timestamps
                    "start_time": self._format_timestamp(segment["start"]),
                    "end_time": self._format_timestamp(segment["end"])
                })

            self.transcript = {
                "full_text": full_text,
                "segments": segments,
                "language": result.get("language", "en"),
                "duration": result["segments"][-1]["end"] if segments else 0.0,
                "filtered_count": filtered_count
            }

            logger.info(f"âœ“ Transcription complete (with hallucination filtering)")
            logger.info(f"  Duration: {self.transcript['duration']:.1f}s ({self._format_timestamp(self.transcript['duration'])})")
            logger.info(f"  Segments: {len(segments)}")
            logger.info(f"  Filtered hallucinations: {filtered_count} segments")
            logger.info(f"  Language: {self.transcript['language']}")
            logger.info(f"  Word-level timestamps: {'âœ“ ENABLED' if word_timestamps else 'âœ— DISABLED'}")
            logger.info(f"  Text preview: {full_text[:150]}...")

            return self.transcript

        except ImportError:
            logger.error("Whisper not installed. Run: pip install openai-whisper")
            raise
        except Exception as e:
            logger.error(f"Whisper transcription error: {e}")
            raise

    def diarize_speakers(self, audio_path: Optional[Path] = None) -> Dict:
        """
        Identify speakers using pyannote.audio

        Args:
            audio_path: Path to audio file (default: use extracted audio)

        Returns:
            Diarization dict with speaker segments
        """
        logger.info("=" * 80)
        logger.info("SPEAKER DIARIZATION WITH PYANNOTE")
        logger.info("=" * 80)

        if audio_path is None:
            if self.audio_path is None:
                raise ValueError("No audio file available. Run extract_audio() first.")
            audio_path = self.audio_path

        try:
            from pyannote.audio import Pipeline
            import os

            # Load pretrained pipeline
            logger.info("Loading pyannote pipeline...")

            # Get HuggingFace token from environment
            hf_token = os.getenv('HF_TOKEN')
            if hf_token:
                logger.info("Using HuggingFace token for speaker diarization")
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=hf_token
                )
            else:
                logger.warning("No HF_TOKEN found - speaker diarization may fail")
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=True
                )

            # Run diarization
            logger.info(f"Diarizing: {audio_path.name}")
            diarization = pipeline(str(audio_path))

            # Extract speaker segments
            speaker_segments = []
            speaker_set = set()

            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker,
                    # Add HH:MM:SS formatted timestamps
                    "start_time": self._format_timestamp(turn.start),
                    "end_time": self._format_timestamp(turn.end)
                })
                speaker_set.add(speaker)

            self.diarization = {
                "segments": speaker_segments,
                "speaker_count": len(speaker_set),
                "speakers": sorted(list(speaker_set))
            }

            logger.info(f"âœ“ Diarization complete")
            logger.info(f"  Speakers detected: {len(speaker_set)}")
            logger.info(f"  Speakers: {', '.join(sorted(speaker_set))}")
            logger.info(f"  Speaker segments: {len(speaker_segments)}")

            return self.diarization

        except ImportError:
            logger.warning("âš ï¸  pyannote not installed. Skipping speaker diarization.")
            logger.warning("   Install with: pip install pyannote-audio")

            # Return mock diarization with multiple speakers using silence detection
            self.diarization = self._fallback_speaker_detection(audio_path)
            return self.diarization

        except Exception as e:
            logger.error(f"Speaker diarization error: {e}")
            logger.warning("âš ï¸  Using fallback speaker detection")

            # Return mock diarization with fallback detection
            self.diarization = self._fallback_speaker_detection(audio_path)
            return self.diarization

    def _fallback_speaker_detection(self, audio_path) -> Dict:
        """
        Fallback speaker detection using silence detection and volume changes.

        When PyAnnotate is unavailable, detects speaker turns by analyzing:
        - Silence gaps (speaker transitions)
        - Volume changes (different mic positions)
        - Pitch frequency differences

        Returns:
            Dict with detected speakers and segments
        """
        try:
            import librosa
            import numpy as np

            logger.info("Analyzing audio for speaker changes using fallback method...")
            y, sr = librosa.load(str(audio_path), sr=16000)

            # Detect silence frames
            S = librosa.feature.melspectrogram(y=y, sr=sr)
            S_db = librosa.power_to_db(S, ref=np.max)

            # RMS energy for each frame
            frame_energy = np.sqrt(np.mean(S**2, axis=0))

            # Detect silence with adaptive threshold
            silence_threshold = np.mean(frame_energy) * 0.3
            silence_frames = frame_energy < silence_threshold

            # Find silence gaps (potential speaker transitions)
            hop_length = 512
            frame_times = librosa.frames_to_time(np.arange(len(silence_frames)), sr=sr)

            # Detect speaker transitions (silence â†’ sound or sound changes)
            speaker_segments = []
            current_speaker_id = 0
            in_speech = False
            last_speaker_change = 0

            for i, is_silent in enumerate(silence_frames):
                time = frame_times[i]

                if not is_silent and not in_speech:
                    # Speaker started talking
                    in_speech = True
                    if time - last_speaker_change > 0.5:  # Minimum 0.5s between speakers
                        current_speaker_id += 1
                        last_speaker_change = time
                elif is_silent and in_speech:
                    # Speaker finished talking
                    in_speech = False

            # Default to at least 2 speakers (common case)
            num_speakers = max(2, current_speaker_id + 1) if current_speaker_id > 0 else 1

            speakers = [f"SPEAKER_{i:02d}" for i in range(num_speakers)]

            logger.info(f"  Detected ~{num_speakers} speaker(s) using fallback method")
            logger.info(f"  Speakers: {', '.join(speakers)}")

            return {
                "segments": [],
                "speaker_count": num_speakers,
                "speakers": speakers
            }

        except Exception as e:
            logger.warning(f"Fallback detection failed: {e}")
            # Last resort: at least return 2 speakers
            return {
                "segments": [],
                "speaker_count": 2,
                "speakers": ["SPEAKER_00", "SPEAKER_01"]
            }

    def detect_background_music(self) -> List[Dict]:
        """
        Detect presence and characteristics of background music using librosa

        Detects music throughout entire video and classifies as intro/background/outro

        Returns:
            List of music event dicts with timestamps
        """
        logger.info("=" * 80)
        logger.info("DETECTING BACKGROUND MUSIC")
        logger.info("=" * 80)

        if self.audio_path is None:
            raise ValueError("No audio file available. Run extract_audio() first.")

        try:
            import librosa
            import numpy as np

            logger.info(f"Analyzing: {self.audio_path.name}")
            y, sr = librosa.load(str(self.audio_path), sr=16000)
            duration = len(y) / sr

            # Compute harmonic/percussive separation
            harmonic, percussive = librosa.effects.hpss(y)

            music_events = []

            # Analyze in 5-second windows
            hop_length = sr * 5  # 5 second windows

            for i in range(0, len(y), hop_length):
                window = y[i:i+hop_length]
                window_harmonic = harmonic[i:i+hop_length]

                if len(window) < sr:  # Skip if too short
                    continue

                # Calculate music likelihood via harmonic ratio
                window_harmonic_ratio = np.sum(np.abs(window_harmonic)) / (np.sum(np.abs(window)) + 1e-10)

                if window_harmonic_ratio > 0.3:  # Music detected
                    start_time = i / sr
                    end_time = min((i + hop_length) / sr, len(y) / sr)

                    # Classify music position (intro/middle/outro)
                    is_intro = start_time < self.MUSIC_DETECTION_WINDOW
                    is_outro = end_time > (duration - self.MUSIC_DETECTION_WINDOW)

                    # Determine subtype
                    if is_intro:
                        subtype = "intro"
                    elif is_outro:
                        subtype = "outro"
                    else:
                        subtype = "background"

                    # Detect tempo
                    tempo, _ = librosa.beat.beat_track(y=window, sr=sr)

                    music_events.append({
                        "type": "background_music",
                        "subtype": subtype,
                        "start": start_time,
                        "end": end_time,
                        "start_time": self._format_timestamp(start_time),
                        "end_time": self._format_timestamp(end_time),
                        "confidence": min(float(window_harmonic_ratio), 1.0),
                        "characteristics": {
                            "tempo": float(tempo),
                            "harmonic_ratio": float(window_harmonic_ratio)
                        }
                    })

            logger.info(f"âœ“ Music detection complete")
            logger.info(f"  Music segments found: {len(music_events)} (intro/background/outro)")

            return music_events

        except ImportError:
            logger.warning("âš ï¸  librosa not installed. Skipping music detection.")
            logger.warning("   Install with: pip install librosa")
            return []
        except Exception as e:
            logger.error(f"Music detection error: {e}")
            return []

    def detect_sound_effects(self) -> List[Dict]:
        """
        Detect discrete sound effects (impacts) using onset detection
        
        OPTIMIZED: Higher RMS threshold (0.20), minimum duration (0.5s), skip "click" type

        Returns:
            List of sound effect event dicts
        """
        logger.info("=" * 80)
        logger.info("DETECTING SOUND EFFECTS (OPTIMIZED)")
        logger.info("=" * 80)

        if self.audio_path is None:
            raise ValueError("No audio file available. Run extract_audio() first.")

        try:
            import librosa
            import numpy as np

            logger.info(f"Analyzing: {self.audio_path.name}")
            y, sr = librosa.load(str(self.audio_path), sr=16000)

            # Detect onsets (sudden energy increases)
            onset_frames = librosa.onset.onset_detect(
                y=y,
                sr=sr,
                hop_length=512,
                backtrack=False,
                units='frames'
            )

            onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=512)

            sound_effects = []

            for onset_time in onset_times:
                # Extract window around onset
                start_sample = int(max(0, (onset_time - 0.1) * sr))
                end_sample = int(min(len(y), (onset_time + 0.5) * sr))
                window = y[start_sample:end_sample]

                # Analyze characteristics
                rms = librosa.feature.rms(y=window)[0]
                zcr = librosa.feature.zero_crossing_rate(window)[0]

                # Classify sound type based on features
                avg_rms = np.mean(rms)
                avg_zcr = np.mean(zcr)
                
                # OPTIMIZATION: Higher RMS threshold (skip low-intensity sounds)
                if avg_rms > self.SOUND_EFFECT_MIN_RMS:
                    sound_type = "impact" if avg_zcr < 0.1 else "click"
                    
                    # OPTIMIZATION: Skip "click" type entirely (too noisy)
                    if sound_type == "click":
                        continue

                    start_time = float(onset_time - 0.1)
                    end_time = float(onset_time + 0.2)
                    
                    # OPTIMIZATION: Check minimum duration
                    duration = end_time - start_time
                    if duration < self.SOUND_EFFECT_MIN_DURATION:
                        continue

                    sound_effects.append({
                        "type": "sound_effect",
                        "subtype": sound_type,
                        "start": start_time,
                        "end": end_time,
                        "start_time": self._format_timestamp(start_time),
                        "end_time": self._format_timestamp(end_time),
                        "confidence": min(float(avg_rms * 2), 1.0),
                        "characteristics": {
                            "intensity": "high" if avg_rms > 0.2 else "medium",
                            "zero_crossing_rate": float(avg_zcr)
                        }
                    })

            logger.info(f"âœ“ Sound effect detection complete (OPTIMIZED)")
            logger.info(f"  Sound effects found: {len(sound_effects)} (high-intensity impacts only)")

            return sound_effects

        except ImportError:
            logger.warning("âš ï¸  librosa not installed. Skipping sound effect detection.")
            return []
        except Exception as e:
            logger.error(f"Sound effect detection error: {e}")
            return []

    def detect_crowd_sounds(self, segments: List[Dict]) -> List[Dict]:
        """
        Detect applause, cheering, laughter in gaps between speech
        
        OPTIMIZED: Higher RMS threshold (0.15) for significant crowd sounds only

        Args:
            segments: Speech segments from transcript

        Returns:
            List of crowd sound event dicts
        """
        logger.info("=" * 80)
        logger.info("DETECTING CROWD SOUNDS (OPTIMIZED)")
        logger.info("=" * 80)

        if self.audio_path is None:
            raise ValueError("No audio file available. Run extract_audio() first.")

        try:
            import librosa
            import numpy as np

            logger.info(f"Analyzing: {self.audio_path.name}")
            y, sr = librosa.load(str(self.audio_path), sr=16000)

            crowd_events = []

            # Look for non-speech segments (gaps between speech)
            for i in range(len(segments) - 1):
                seg1 = segments[i]
                seg2 = segments[i + 1]

                gap_start = seg1["end"]
                gap_end = seg2["start"]
                gap_duration = gap_end - gap_start

                if gap_duration > 0.5:  # Significant gap
                    # Extract audio from gap
                    start_sample = int(gap_start * sr)
                    end_sample = int(gap_end * sr)
                    window = y[start_sample:end_sample]

                    # Analyze spectral characteristics
                    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=window, sr=sr))
                    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=window, sr=sr))
                    rms = np.mean(librosa.feature.rms(y=window))

                    # OPTIMIZATION: Higher RMS threshold for significant crowd sounds
                    # Crowd sounds have broad frequency spectrum + high energy
                    if spectral_bandwidth > 2000 and rms > self.CROWD_SOUND_MIN_RMS:
                        crowd_events.append({
                            "type": "crowd_sound",
                            "subtype": "applause",  # Could classify further
                            "start": gap_start,
                            "end": gap_end,
                            "start_time": self._format_timestamp(gap_start),
                            "end_time": self._format_timestamp(gap_end),
                            "confidence": min(float(rms * 5), 1.0),
                            "characteristics": {
                                "intensity": "loud" if rms > 0.2 else "medium",
                                "spectral_bandwidth": float(spectral_bandwidth)
                            }
                        })

            logger.info(f"âœ“ Crowd sound detection complete (OPTIMIZED)")
            logger.info(f"  Crowd sounds found: {len(crowd_events)} (high-intensity only)")

            return crowd_events

        except ImportError:
            logger.warning("âš ï¸  librosa not installed. Skipping crowd sound detection.")
            return []
        except Exception as e:
            logger.error(f"Crowd sound detection error: {e}")
            return []

    def detect_music_changes(self, music_segments: List[Dict]) -> List[Dict]:
        """
        Detect tempo changes, pitch shifts within music segments
        
        OPTIMIZED: Only tempo changes > 20 BPM (significant pacing changes)

        Args:
            music_segments: Music segments from detect_background_music()

        Returns:
            List of music change event dicts
        """
        logger.info("=" * 80)
        logger.info("DETECTING MUSIC CHANGES (OPTIMIZED)")
        logger.info("=" * 80)

        if self.audio_path is None:
            raise ValueError("No audio file available. Run extract_audio() first.")

        if not music_segments:
            logger.info("  No music segments to analyze")
            return []

        try:
            import librosa
            import numpy as np

            logger.info(f"Analyzing: {self.audio_path.name}")
            y, sr = librosa.load(str(self.audio_path), sr=16000)

            changes = []

            for music_seg in music_segments:
                start_sample = int(music_seg["start"] * sr)
                end_sample = int(music_seg["end"] * sr)
                music_audio = y[start_sample:end_sample]

                # Divide into sub-windows to detect changes
                window_size = sr * 5  # 5-second windows

                prev_tempo = None

                for i in range(0, len(music_audio), window_size):
                    window = music_audio[i:i+window_size]

                    if len(window) < sr:  # Too short
                        continue

                    # Get tempo
                    tempo, _ = librosa.beat.beat_track(y=window, sr=sr)

                    # Detect changes
                    if prev_tempo is not None:
                        tempo_change = abs(tempo - prev_tempo)
                        
                        # OPTIMIZATION: Only significant tempo changes (> 20 BPM)
                        if tempo_change > self.MUSIC_MIN_TEMPO_CHANGE:
                            change_time = music_seg["start"] + (i / sr)
                            changes.append({
                                "type": "music_change",
                                "subtype": "tempo_increase" if tempo > prev_tempo else "tempo_decrease",
                                "start": change_time,
                                "end": change_time + 0.5,
                                "start_time": self._format_timestamp(change_time),
                                "end_time": self._format_timestamp(change_time + 0.5),
                                "confidence": min(float(tempo_change / 50), 1.0),
                                "characteristics": {
                                    "before_tempo": float(prev_tempo),
                                    "after_tempo": float(tempo),
                                    "change_magnitude": float(tempo_change)
                                }
                            })

                    prev_tempo = tempo

            logger.info(f"âœ“ Music change detection complete (OPTIMIZED)")
            logger.info(f"  Music changes found: {len(changes)} (tempo changes > {self.MUSIC_MIN_TEMPO_CHANGE} BPM)")

            return changes

        except ImportError:
            logger.warning("âš ï¸  librosa not installed. Skipping music change detection.")
            return []
        except Exception as e:
            logger.error(f"Music change detection error: {e}")
            return []

    def detect_silence_gaps(self, threshold_db: float = -40) -> List[Dict]:
        """
        Detect silence gaps that might indicate scene changes

        Args:
            threshold_db: Silence threshold in dB (default: -40)

        Returns:
            List of silence gap dicts
        """
        logger.info("=" * 80)
        logger.info("DETECTING SILENCE GAPS")
        logger.info("=" * 80)

        if self.audio_path is None:
            raise ValueError("No audio file available. Run extract_audio() first.")

        try:
            import librosa
            import numpy as np

            logger.info(f"Analyzing: {self.audio_path.name}")
            y, sr = librosa.load(str(self.audio_path), sr=16000)

            # Convert to dB
            db = librosa.amplitude_to_db(np.abs(y), ref=np.max)

            # Find silent regions
            silent = db < threshold_db

            # Group consecutive silent frames
            gaps = []
            in_gap = False
            gap_start = None

            # BUG FIX: Convert sample indices to seconds directly (not frame indices)
            # Previous code used librosa.frames_to_time() which expects STFT frames,
            # but we're working with raw samples here
            times = np.arange(len(silent)) / sr  # Convert sample indices to seconds

            for i, is_silent in enumerate(silent):
                if is_silent and not in_gap:
                    gap_start = times[i]
                    in_gap = True
                elif not is_silent and in_gap:
                    gap_end = times[i]
                    duration = gap_end - gap_start

                    if duration > 0.3:  # Minimum 0.3s silence
                        gaps.append({
                            "start": gap_start,
                            "end": gap_end,
                            "start_time": self._format_timestamp(gap_start),
                            "end_time": self._format_timestamp(gap_end),
                            "duration": duration,
                            "type": "scene_change" if duration > 1.5 else "pause"
                        })

                    in_gap = False

            logger.info(f"âœ“ Silence detection complete")
            logger.info(f"  Silence gaps found: {len(gaps)}")
            logger.info(f"  Scene changes (>1.5s): {len([g for g in gaps if g['type'] == 'scene_change'])}")

            return gaps

        except ImportError:
            logger.warning("âš ï¸  librosa not installed. Skipping silence detection.")
            return []
        except Exception as e:
            logger.error(f"Silence detection error: {e}")
            return []

    def combine_transcript_speakers(self) -> Dict:
        """
        Merge Whisper transcript with pyannote speaker labels

        Returns:
            Combined dict with transcript segments labeled by speaker
        """
        logger.info("=" * 80)
        logger.info("COMBINING TRANSCRIPT + SPEAKER LABELS")
        logger.info("=" * 80)

        if self.transcript is None:
            raise ValueError("No transcript available. Run transcribe_with_whisper() first.")

        if self.diarization is None:
            raise ValueError("No diarization available. Run diarize_speakers() first.")

        # If no speaker segments (pyannote failed), assign default speaker
        if not self.diarization["segments"]:
            logger.warning("No speaker diarization available, assigning default speaker")
            for segment in self.transcript["segments"]:
                segment["speaker"] = "SPEAKER_00"
        else:
            # Match transcript segments to speaker segments
            for segment in self.transcript["segments"]:
                # Find overlapping speaker (by midpoint of transcript segment)
                midpoint = (segment["start"] + segment["end"]) / 2
                speaker = self._find_speaker_at_time(midpoint)
                segment["speaker"] = speaker

        combined = {
            "transcript": self.transcript["full_text"],
            "segments": self.transcript["segments"],
            "speaker_count": self.diarization["speaker_count"],
            "speakers": self.diarization["speakers"],
            "duration": self.transcript["duration"],
            "language": self.transcript["language"]
        }

        logger.info(f"âœ“ Combination complete")
        logger.info(f"  Total segments: {len(combined['segments'])}")
        logger.info(f"  Speakers: {combined['speaker_count']}")

        # Show sample segments
        logger.info("\nSample segments:")
        for i, seg in enumerate(combined["segments"][:5]):
            time_str = self._format_timestamp(seg["start"])
            logger.info(f"  [{time_str}] {seg['speaker']}: {seg['text'][:60]}...")

        return combined

    def _find_speaker_at_time(self, timestamp: float) -> str:
        """
        Find which speaker is active at given timestamp

        Args:
            timestamp: Time in seconds

        Returns:
            Speaker label (e.g., "SPEAKER_01")
        """
        if not self.diarization or not self.diarization["segments"]:
            return "SPEAKER_00"

        # Find first speaker segment that contains this timestamp
        for seg in self.diarization["segments"]:
            if seg["start"] <= timestamp <= seg["end"]:
                return seg["speaker"]

        # If no exact match, find closest
        closest_speaker = "SPEAKER_00"
        min_distance = float('inf')

        for seg in self.diarization["segments"]:
            # Distance to start of segment
            distance = abs(seg["start"] - timestamp)
            if distance < min_distance:
                min_distance = distance
                closest_speaker = seg["speaker"]

        return closest_speaker

    def _format_timestamp(self, seconds: float) -> str:
        """
        Format seconds as HH:MM:SS

        Args:
            seconds: Time in seconds

        Returns:
            Formatted time string (HH:MM:SS)
        """
        td = timedelta(seconds=seconds)
        hours = td.seconds // 3600
        minutes = (td.seconds % 3600) // 60
        secs = td.seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    def analyze(self, save_json: bool = True, output_dir: Optional[Path] = None) -> Dict:
        """
        Complete audio analysis pipeline with optimized audio event detection

        Args:
            save_json: Save results to JSON file
            output_dir: Directory to save audio file (default: temp dir)

        Returns:
            Complete analysis results with segments, audio events, and silence gaps
        """
        logger.info("=" * 80)
        logger.info("STARTING COMPLETE AUDIO ANALYSIS (OPTIMIZED)")
        logger.info("=" * 80)

        # Step 1: Extract audio to specified directory
        self.extract_audio(output_dir=output_dir)

        # Step 2: Transcribe with word-level timestamps
        self.transcribe_with_whisper()

        # Step 3: Diarize speakers
        self.diarize_speakers()

        # Step 4: Combine transcript + speakers
        combined = self.combine_transcript_speakers()

        # Step 5: OPTIMIZED - Detect background music (intro/outro only)
        logger.info("\nðŸŽµ Detecting background music (OPTIMIZED)...")
        music_events = self.detect_background_music()

        # Step 6: OPTIMIZED - Detect sound effects (high-intensity impacts only)
        logger.info("\nðŸ”Š Detecting sound effects (OPTIMIZED)...")
        sound_effects = self.detect_sound_effects()

        # Step 7: OPTIMIZED - Detect crowd sounds (high-intensity only)
        logger.info("\nðŸ‘¥ Detecting crowd sounds (OPTIMIZED)...")
        crowd_sounds = self.detect_crowd_sounds(combined["segments"])

        # Step 8: OPTIMIZED - Detect music changes (tempo > 20 BPM)
        logger.info("\nðŸŽ¼ Detecting music changes (OPTIMIZED)...")
        music_changes = self.detect_music_changes(music_events)

        # Step 9: Detect silence gaps (scene changes)
        logger.info("\nðŸ”‡ Detecting silence gaps...")
        silence_gaps = self.detect_silence_gaps()

        # Combine all audio events
        audio_events = (
            music_events +
            sound_effects +
            crowd_sounds +
            music_changes
        )

        # Sort by timestamp
        audio_events.sort(key=lambda x: x["start"])

        # Build final results with enhanced structure
        results = {
            "video_id": self.video_path.stem,
            "duration": combined["duration"],
            "duration_formatted": self._format_timestamp(combined["duration"]),
            "language": combined["language"],
            "transcript": combined["transcript"],
            "segments": combined["segments"],  # Now includes word-level timestamps
            "audio_events": audio_events,       # OPTIMIZED: Quality over quantity
            "silence_gaps": silence_gaps,       # Scene change indicators
            "speaker_count": combined["speaker_count"],
            "speakers": combined["speakers"]
        }

        # Save to JSON if requested
        if save_json:
            output_path = self.video_path.parent / f"{self.video_path.stem}_audio_analysis.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ“ Saved analysis to: {output_path}")

        logger.info("=" * 80)
        logger.info("âœ… AUDIO ANALYSIS COMPLETE (OPTIMIZED)")
        logger.info("=" * 80)
        logger.info(f"  Duration: {results['duration']:.1f}s ({results['duration_formatted']})")
        logger.info(f"  Segments: {len(results['segments'])}")
        logger.info(f"  Audio Events: {len(results['audio_events'])} (OPTIMIZED - quality over quantity)")
        logger.info(f"    - Music segments: {len(music_events)} (intro/outro + tempo changes)")
        logger.info(f"    - Sound effects: {len(sound_effects)} (high-intensity impacts)")
        logger.info(f"    - Crowd sounds: {len(crowd_sounds)} (high-intensity)")
        logger.info(f"    - Music changes: {len(music_changes)} (tempo > {self.MUSIC_MIN_TEMPO_CHANGE} BPM)")
        logger.info(f"  Silence Gaps: {len(results['silence_gaps'])}")
        logger.info(f"  Speakers: {results['speaker_count']}")
        logger.info("=" * 80)

        return results


def test_audio_analyzer(video_path: str):
    """
    Test function for audio analysis

    Args:
        video_path: Path to test video
    """
    analyzer = AudioAnalyzer(video_path)
    results = analyzer.analyze(save_json=True)

    print("\n" + "=" * 80)
    print("AUDIO ANALYSIS RESULTS (OPTIMIZED)")
    print("=" * 80)
    print(f"Duration: {results['duration']:.1f}s ({results['duration_formatted']})")
    print(f"Language: {results['language']}")
    print(f"Speakers: {results['speaker_count']}")
    print(f"Segments: {len(results['segments'])}")
    print(f"Audio Events: {len(results['audio_events'])} (OPTIMIZED)")
    print(f"Silence Gaps: {len(results['silence_gaps'])}")
    print("\nFull Transcript:")
    print("-" * 80)
    print(results['transcript'][:500] + "...")
    print("\nSegments with Speakers:")
    print("-" * 80)
    for i, seg in enumerate(results['segments'][:10], 1):
        print(f"{i}. [{seg['start_time']}] {seg['speaker']}")
        print(f"   {seg['text']}")
        if seg['words']:
            print(f"   Word count: {len(seg['words'])}")
    print("\nAudio Events (OPTIMIZED):")
    print("-" * 80)
    for i, event in enumerate(results['audio_events'][:10], 1):
        print(f"{i}. [{event['start_time']} - {event['end_time']}] {event['type']}/{event['subtype']}")
    print("=" * 80)


if __name__ == "__main__":
    # Test with video
    import sys
    if len(sys.argv) > 1:
        test_audio_analyzer(sys.argv[1])
    else:
        print("Usage: python audio_analysis_optimized.py <video_path>")